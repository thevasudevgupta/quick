{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_train_mbart.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJbYXou6chZf","executionInfo":{"status":"ok","timestamp":1618658728640,"user_tz":-330,"elapsed":1005,"user":{"displayName":"Vasudev Gupta me18b182","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoD5Ek5C5fPA8MX5Is5W-ZFFHkEup62P_vnE6A8w=s64","userId":"11410978576234375819"}},"outputId":"c861b4cb-ef4a-43fe-8509-d064405bf7ea"},"source":["!nvidia-smi"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Sat Apr 17 11:25:28 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   50C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xZN78wWxkUzb","executionInfo":{"status":"ok","timestamp":1618656748125,"user_tz":-330,"elapsed":1790,"user":{"displayName":"Vasudev Gupta me18b182","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoD5Ek5C5fPA8MX5Is5W-ZFFHkEup62P_vnE6A8w=s64","userId":"11410978576234375819"}},"outputId":"389109c6-27ee-4241-e18d-c50a3b393942"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wbq88ramcln3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618656748126,"user_tz":-330,"elapsed":1780,"user":{"displayName":"Vasudev Gupta me18b182","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoD5Ek5C5fPA8MX5Is5W-ZFFHkEup62P_vnE6A8w=s64","userId":"11410978576234375819"}},"outputId":"2b8499d0-012e-4f28-b1e2-8d9bb95d4885"},"source":["cd /content/drive/MyDrive/quick"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/quick\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KMO0VZeClBXu","executionInfo":{"status":"ok","timestamp":1618656760471,"user_tz":-330,"elapsed":14116,"user":{"displayName":"Vasudev Gupta me18b182","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoD5Ek5C5fPA8MX5Is5W-ZFFHkEup62P_vnE6A8w=s64","userId":"11410978576234375819"}}},"source":["%%capture\n","!pip3 install git+https://vasudevgupta7:c705358b82137c0d7d0203fc91672a55d02112cf@github.com/vasudevgupta7/workstation@main\n","!pip3 install transformers\n","!pip3 install datasets\n","!pip3 install sentencepiece"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ViErKUiIUnrI","executionInfo":{"status":"ok","timestamp":1618656781919,"user_tz":-330,"elapsed":35563,"user":{"displayName":"Vasudev Gupta me18b182","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoD5Ek5C5fPA8MX5Is5W-ZFFHkEup62P_vnE6A8w=s64","userId":"11410978576234375819"}},"outputId":"8ca679a0-aeee-47a3-97b8-38e91b075cdf"},"source":["!ws --init"],"execution_count":5,"outputs":[{"output_type":"stream","text":["INSTALLING numpy ... ‚úÖ\n","INSTALLING tensorboard ... ‚úÖ\n","INSTALLING wandb ... ‚úÖ\n","INSTALLING tqdm ... ‚úÖ\n","FAILED üôÅ\n","INSTALLING  ... INSTALLING torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html ... ‚úÖ\n","FAILED üôÅ\n","INSTALLING  ... Setting Up ü§ó Hub ... ‚úÖ\n","Setting Up wandb ... ‚úÖ\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MuYcHSanRXpq"},"source":["!pip3 install deepspeed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKphbDBaRXpr","executionInfo":{"status":"ok","timestamp":1618657327285,"user_tz":-330,"elapsed":8620,"user":{"displayName":"Vasudev Gupta me18b182","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoD5Ek5C5fPA8MX5Is5W-ZFFHkEup62P_vnE6A8w=s64","userId":"11410978576234375819"}},"outputId":"eba99f61-9bc0-4df3-98c5-7ac8540934bf"},"source":["!ds_report"],"execution_count":15,"outputs":[{"output_type":"stream","text":["--------------------------------------------------\n","DeepSpeed C++/CUDA extension op report\n","--------------------------------------------------\n","NOTE: Ops not installed will be just-in-time (JIT) compiled at\n","      runtime if needed. Op compatibility means that your system\n","      meet the required dependencies to JIT install the op.\n","--------------------------------------------------\n","JIT compiled ops requires ninja\n","ninja .................. \u001b[92m[OKAY]\u001b[0m\n","--------------------------------------------------\n","op name ................ installed .. compatible\n","--------------------------------------------------\n","cpu_adam ............... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n","fused_adam ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n","fused_lamb ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires one of the following commands '['llvm-config', 'llvm-config-9']', but it does not exist!\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires CUDA version 10.1+, does not currently support >=11 or <10.1\n","sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n","transformer ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n","stochastic_transformer . \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n","utils .................. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n","--------------------------------------------------\n","DeepSpeed general environment info:\n","torch install path ............... ['/usr/local/lib/python3.7/dist-packages/torch']\n","torch version .................... 1.7.1+cu110\n","torch cuda version ............... 11.0\n","nvcc version ..................... 11.0\n","deepspeed install path ........... ['/usr/local/lib/python3.7/dist-packages/deepspeed']\n","deepspeed info ................... 0.3.14, unknown, unknown\n","deepspeed wheel compiled w. ...... torch 1.8, cuda 10.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ah693oZIubtR","executionInfo":{"status":"ok","timestamp":1618656787330,"user_tz":-330,"elapsed":40957,"user":{"displayName":"Vasudev Gupta me18b182","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoD5Ek5C5fPA8MX5Is5W-ZFFHkEup62P_vnE6A8w=s64","userId":"11410978576234375819"}}},"source":["%%capture\n","!pip install -e \".\""],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"7PAoR1e5ZRxR"},"source":["# CUDA 11.0\n","# !pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AILwb1f_pv03"},"source":["# !ENABLE_DEEPSPEED=False FILE_PATH=\"examples/clean_article.csv\" python3 examples/train_mbart.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H7jLn0sy3Yem","colab":{"base_uri":"https://localhost:8080/"},"outputId":"daf52d63-3797-40cc-99b4-6ab9b38a2ebc"},"source":["!ENABLE_DEEPSPEED=True FILE_PATH=\"examples/clean_article.csv\" deepspeed examples/train_mbart.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2021-04-17 11:40:34,402] [WARNING] [runner.py:117:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n","[2021-04-17 11:40:34,433] [INFO] [runner.py:358:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 examples/train_mbart.py\n","[2021-04-17 11:40:35,546] [INFO] [launch.py:73:main] 0 NCCL_VERSION 2.7.8\n","[2021-04-17 11:40:35,547] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n","[2021-04-17 11:40:35,547] [INFO] [launch.py:89:main] nnodes=1, num_local_procs=1, node_rank=0\n","[2021-04-17 11:40:35,547] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n","[2021-04-17 11:40:35,547] [INFO] [launch.py:102:main] dist_world_size=1\n","[2021-04-17 11:40:35,547] [INFO] [launch.py:105:main] Setting CUDA_VISIBLE_DEVICES=0\n","TrainingArgs(batch_size=None, lr=5e-05, gradient_accumulation_steps=1, precision=None, max_epochs=5, output_dir='Quick-project', save_strategy='epoch', project_name='Quick-project', wandb_run_name=None, early_stop_n=None, epoch_saving_n=None, enable_deepspeed=True, deepspeed_plugin=DeepSpeedPlugin(local_rank=0, train_batch_size=1, gradient_accumulation_steps=1, fp16={'enabled': False}, zero_optimization={'stage': 0, 'cpu_offload': False}))\n","Using custom data configuration default-cb030ef1eba95c44\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-cb030ef1eba95c44/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-cb030ef1eba95c44/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-537091de8626ca8e.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-cb030ef1eba95c44/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-fa59ae3086f45620.arrow\n","Dataset({\n","    features: ['Text_ID', 'Text', 'Headline', 'Mobile_Tech_Tag', 'cleaned', 'phonemes'],\n","    num_rows: 3705\n","})\n","Loading cached split indices for dataset at /root/.cache/huggingface/datasets/csv/default-cb030ef1eba95c44/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-733729de6a786fcd.arrow and /root/.cache/huggingface/datasets/csv/default-cb030ef1eba95c44/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-cc8af012215f236f.arrow\n","Dataset({\n","    features: ['Text_ID', 'Text', 'Headline', 'Mobile_Tech_Tag', 'cleaned', 'phonemes'],\n","    num_rows: 3105\n","}) Dataset({\n","    features: ['Text_ID', 'Text', 'Headline', 'Mobile_Tech_Tag', 'cleaned', 'phonemes'],\n","    num_rows: 600\n","})\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m7vasudevgupta\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-04-17 11:42:27.930998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.26\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwhole-mountain-15\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/7vasudevgupta/Quick-project\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/7vasudevgupta/Quick-project/runs/1bfy2tkm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in Quick-project/wandb/run-20210417_114226-1bfy2tkm\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","[2021-04-17 11:42:29,532] [INFO] [logging.py:60:log_dist] [Rank -1] DeepSpeed info: version=0.3.14, git-hash=unknown, git-branch=unknown\n","[2021-04-17 11:42:29,533] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl\n","[2021-04-17 11:42:33,540] [INFO] [engine.py:80:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n","Using /root/.cache/torch_extensions as PyTorch extensions root...\n","Detected CUDA files, patching ldflags\n","Emitting ninja build file /root/.cache/torch_extensions/fused_adam/build.ninja...\n","Building extension module fused_adam...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","ninja: no work to do.\n","Loading extension module fused_adam...\n","Time to load fused_adam op: 0.7017874717712402 seconds\n","[2021-04-17 11:42:35,415] [INFO] [engine.py:608:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n","[2021-04-17 11:42:35,416] [INFO] [engine.py:612:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n","[2021-04-17 11:42:35,416] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n","[2021-04-17 11:42:35,416] [INFO] [engine.py:445:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n","[2021-04-17 11:42:35,416] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f7980195bd0>\n","[2021-04-17 11:42:35,416] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:42:35,416] [INFO] [config.py:737:print] DeepSpeedEngine configuration:\n","[2021-04-17 11:42:35,416] [INFO] [config.py:741:print]   activation_checkpointing_config  {\n","    \"contiguous_memory_optimization\": false,\n","    \"cpu_checkpointing\": false,\n","    \"number_checkpoints\": null,\n","    \"partition_activations\": false,\n","    \"profile\": false,\n","    \"synchronize_checkpoint_boundary\": false\n","}\n","[2021-04-17 11:42:35,416] [INFO] [config.py:741:print]   allreduce_always_fp32 ........ False\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   amp_enabled .................. False\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   amp_params ................... False\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   checkpoint_tag_validation_enabled  True\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   checkpoint_tag_validation_fail  False\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   disable_allgather ............ False\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   dump_state ................... False\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   dynamic_loss_scale_args ...... None\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   elasticity_enabled ........... False\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   flops_profiler_config ........ {\n","    \"detailed\": true,\n","    \"enabled\": false,\n","    \"module_depth\": -1,\n","    \"profile_step\": 1,\n","    \"top_modules\": 3\n","}\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   fp16_enabled ................. False\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   global_rank .................. 0\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   gradient_accumulation_steps .. 1\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   gradient_clipping ............ 0.0\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   gradient_predivide_factor .... 1.0\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   initial_dynamic_scale ........ 4294967296\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   loss_scale ................... 0\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   memory_breakdown ............. False\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   optimizer_legacy_fusion ...... False\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   optimizer_name ............... adam\n","[2021-04-17 11:42:35,417] [INFO] [config.py:741:print]   optimizer_params ............. {'lr': 5e-05, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\n","[2021-04-17 11:42:35,418] [INFO] [config.py:741:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n","[2021-04-17 11:42:35,418] [INFO] [config.py:741:print]   pld_enabled .................. False\n","[2021-04-17 11:42:35,418] [INFO] [config.py:741:print]   pld_params ................... False\n","[2021-04-17 11:42:35,418] [INFO] [config.py:741:print]   prescale_gradients ........... False\n","[2021-04-17 11:42:35,418] [INFO] [config.py:741:print]   scheduler_name ............... WarmupLR\n","[2021-04-17 11:42:35,418] [INFO] [config.py:741:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 7e-05, 'warmup_num_steps': 1000}\n","[2021-04-17 11:42:35,418] [INFO] [config.py:741:print]   sparse_attention ............. None\n","[2021-04-17 11:42:35,418] [INFO] [config.py:741:print]   sparse_gradients_enabled ..... False\n","[2021-04-17 11:42:35,419] [INFO] [config.py:741:print]   steps_per_print .............. 10\n","[2021-04-17 11:42:35,419] [INFO] [config.py:741:print]   tensorboard_enabled .......... False\n","[2021-04-17 11:42:35,419] [INFO] [config.py:741:print]   tensorboard_job_name ......... DeepSpeedJobName\n","[2021-04-17 11:42:35,419] [INFO] [config.py:741:print]   tensorboard_output_path ...... \n","[2021-04-17 11:42:35,419] [INFO] [config.py:741:print]   train_batch_size ............. 1\n","[2021-04-17 11:42:35,419] [INFO] [config.py:741:print]   train_micro_batch_size_per_gpu  1\n","[2021-04-17 11:42:35,419] [INFO] [config.py:741:print]   wall_clock_breakdown ......... False\n","[2021-04-17 11:42:35,419] [INFO] [config.py:741:print]   world_size ................... 1\n","[2021-04-17 11:42:35,419] [INFO] [config.py:741:print]   zero_allow_untested_optimizer  False\n","[2021-04-17 11:42:35,419] [INFO] [config.py:741:print]   zero_config .................. {\n","    \"allgather_bucket_size\": 500000000,\n","    \"allgather_partitions\": true,\n","    \"contiguous_gradients\": false,\n","    \"cpu_offload\": false,\n","    \"cpu_offload_params\": false,\n","    \"cpu_offload_use_pin_memory\": false,\n","    \"elastic_checkpoint\": true,\n","    \"gather_fp16_weights_on_model_save\": false,\n","    \"load_from_fp32_weights\": true,\n","    \"max_live_parameters\": 1000000000,\n","    \"max_reuse_distance\": 1000000000,\n","    \"overlap_comm\": false,\n","    \"param_persistence_threshold\": 100000,\n","    \"prefetch_bucket_size\": 50000000,\n","    \"reduce_bucket_size\": 500000000,\n","    \"reduce_scatter\": true,\n","    \"stage\": 0,\n","    \"sub_group_size\": 1000000000000\n","}\n","[2021-04-17 11:42:35,419] [INFO] [config.py:741:print]   zero_enabled ................. False\n","[2021-04-17 11:42:35,419] [INFO] [config.py:741:print]   zero_optimization_stage ...... 0\n","[2021-04-17 11:42:35,419] [INFO] [config.py:747:print]   json = {\n","    \"fp16\":{\n","        \"enabled\":false\n","    },\n","    \"gradient_accumulation_steps\":1,\n","    \"local_rank\":0,\n","    \"optimizer\":{\n","        \"params\":{\n","            \"betas\":[\n","                0.8,\n","                0.999\n","            ],\n","            \"eps\":1e-08,\n","            \"lr\":5e-05,\n","            \"weight_decay\":3e-07\n","        },\n","        \"type\":\"Adam\"\n","    },\n","    \"scheduler\":{\n","        \"params\":{\n","            \"warmup_max_lr\":7e-05,\n","            \"warmup_min_lr\":1e-05,\n","            \"warmup_num_steps\":1000\n","        },\n","        \"type\":\"WarmupLR\"\n","    },\n","    \"train_batch_size\":1,\n","    \"zero_optimization\":{\n","        \"cpu_offload\":false,\n","        \"stage\":0\n","    }\n","}\n","Using /root/.cache/torch_extensions as PyTorch extensions root...\n","Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n","Building extension module utils...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","ninja: no work to do.\n","Loading extension module utils...\n","Time to load utils op: 0.7268486022949219 seconds\n","running epoch-0:   0%|                                 | 0/3105 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:3226: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ü§ó Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:3226: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ü§ó Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n","  FutureWarning,\n","running epoch-0:   0%|           | 9/3105 [00:03<16:02,  3.22it/s, tr_loss=4.98][2021-04-17 11:42:39,480] [INFO] [logging.py:60:log_dist] [Rank 0] step=10, skipped=0, lr=[3.0000000000000004e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:42:39,630] [INFO] [timer.py:157:stop] 0/10, SamplesPerSec=3.607250960552315\n","running epoch-0:   1%|          | 19/3105 [00:05<14:59,  3.43it/s, tr_loss=1.05][2021-04-17 11:42:42,378] [INFO] [logging.py:60:log_dist] [Rank 0] step=20, skipped=0, lr=[3.602059991327962e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:42:42,541] [INFO] [timer.py:157:stop] 0/20, SamplesPerSec=3.539602255406935\n","running epoch-0:   1%|          | 29/3105 [00:08<15:02,  3.41it/s, tr_loss=4.96][2021-04-17 11:42:45,327] [INFO] [logging.py:60:log_dist] [Rank 0] step=30, skipped=0, lr=[3.954242509439325e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:42:45,484] [INFO] [timer.py:157:stop] 0/30, SamplesPerSec=3.504405254531984\n","running epoch-0:   1%|‚ñè         | 39/3105 [00:11<15:05,  3.39it/s, tr_loss=4.07][2021-04-17 11:42:48,274] [INFO] [logging.py:60:log_dist] [Rank 0] step=40, skipped=0, lr=[4.204119982655924e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:42:48,441] [INFO] [timer.py:157:stop] 0/40, SamplesPerSec=3.4855995983405084\n","running epoch-0:   2%|‚ñè         | 49/3105 [00:14<15:01,  3.39it/s, tr_loss=4.72][2021-04-17 11:42:51,213] [INFO] [logging.py:60:log_dist] [Rank 0] step=50, skipped=0, lr=[4.397940008672037e-05], mom=[[0.8, 0.999]]\n","running epoch-0:   2%|‚ñè         | 50/3105 [00:15<14:59,  3.40it/s, tr_loss=2.76][2021-04-17 11:42:51,380] [INFO] [timer.py:157:stop] 0/50, SamplesPerSec=3.483491650353258\n","running epoch-0:   2%|‚ñè         | 59/3105 [00:17<14:53,  3.41it/s, tr_loss=4.79][2021-04-17 11:42:54,153] [INFO] [logging.py:60:log_dist] [Rank 0] step=60, skipped=0, lr=[4.556302500767287e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:42:54,318] [INFO] [timer.py:157:stop] 0/60, SamplesPerSec=3.479336020913635\n","running epoch-0:   2%|‚ñè        | 69/3105 [00:20<15:05,  3.35it/s, tr_loss=0.836][2021-04-17 11:42:57,114] [INFO] [logging.py:60:log_dist] [Rank 0] step=70, skipped=0, lr=[4.6901960800285134e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:42:57,257] [INFO] [timer.py:157:stop] 0/70, SamplesPerSec=3.4761839085772075\n","running epoch-0:   3%|‚ñé         | 79/3105 [00:23<15:00,  3.36it/s, tr_loss=5.38][2021-04-17 11:43:00,048] [INFO] [logging.py:60:log_dist] [Rank 0] step=80, skipped=0, lr=[4.8061799739838866e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:00,215] [INFO] [timer.py:157:stop] 0/80, SamplesPerSec=3.4695862167258418\n","running epoch-0:   3%|‚ñé         | 89/3105 [00:26<14:43,  3.41it/s, tr_loss=4.53][2021-04-17 11:43:02,947] [INFO] [logging.py:60:log_dist] [Rank 0] step=90, skipped=0, lr=[4.908485018878649e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:03,112] [INFO] [timer.py:157:stop] 0/90, SamplesPerSec=3.47466966662244\n","running epoch-0:   3%|‚ñé          | 99/3105 [00:29<14:47,  3.39it/s, tr_loss=1.4][2021-04-17 11:43:05,899] [INFO] [logging.py:60:log_dist] [Rank 0] step=100, skipped=0, lr=[5e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:06,066] [INFO] [timer.py:157:stop] 0/100, SamplesPerSec=3.4719475605759675\n","running epoch-0:   4%|‚ñé        | 109/3105 [00:32<14:46,  3.38it/s, tr_loss=3.25][2021-04-17 11:43:08,831] [INFO] [logging.py:60:log_dist] [Rank 0] step=110, skipped=0, lr=[5.08278537031645e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:08,997] [INFO] [timer.py:157:stop] 0/110, SamplesPerSec=3.472137258043717\n","running epoch-0:   4%|‚ñé        | 119/3105 [00:35<14:49,  3.36it/s, tr_loss=4.11][2021-04-17 11:43:11,782] [INFO] [logging.py:60:log_dist] [Rank 0] step=120, skipped=0, lr=[5.1583624920952494e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:11,930] [INFO] [timer.py:157:stop] 0/120, SamplesPerSec=3.471398500628274\n","running epoch-0:   4%|‚ñç           | 129/3105 [00:38<14:35,  3.40it/s, tr_loss=3][2021-04-17 11:43:14,705] [INFO] [logging.py:60:log_dist] [Rank 0] step=130, skipped=0, lr=[5.227886704613673e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:14,875] [INFO] [timer.py:157:stop] 0/130, SamplesPerSec=3.4697540072223525\n","running epoch-0:   4%|‚ñç        | 139/3105 [00:41<14:42,  3.36it/s, tr_loss=2.98][2021-04-17 11:43:17,688] [INFO] [logging.py:60:log_dist] [Rank 0] step=140, skipped=0, lr=[5.292256071356476e-05], mom=[[0.8, 0.999]]\n","running epoch-0:   5%|‚ñç        | 140/3105 [00:41<14:48,  3.34it/s, tr_loss=2.05][2021-04-17 11:43:17,857] [INFO] [timer.py:157:stop] 0/140, SamplesPerSec=3.46604018203051\n","running epoch-0:   5%|‚ñç        | 149/3105 [00:44<14:23,  3.42it/s, tr_loss=3.53][2021-04-17 11:43:20,626] [INFO] [logging.py:60:log_dist] [Rank 0] step=150, skipped=0, lr=[5.3521825181113616e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:20,796] [INFO] [timer.py:157:stop] 0/150, SamplesPerSec=3.4654645048485646\n","running epoch-0:   5%|‚ñç        | 159/3105 [00:47<14:29,  3.39it/s, tr_loss=1.23][2021-04-17 11:43:23,597] [INFO] [logging.py:60:log_dist] [Rank 0] step=160, skipped=0, lr=[5.408239965311849e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:23,764] [INFO] [timer.py:157:stop] 0/160, SamplesPerSec=3.4632110145761774\n","running epoch-0:   5%|‚ñç        | 169/3105 [00:50<14:26,  3.39it/s, tr_loss=2.02][2021-04-17 11:43:26,555] [INFO] [logging.py:60:log_dist] [Rank 0] step=170, skipped=0, lr=[5.460897842756548e-05], mom=[[0.8, 0.999]]\n","running epoch-0:   5%|‚ñå         | 170/3105 [00:50<14:24,  3.39it/s, tr_loss=2.6][2021-04-17 11:43:26,722] [INFO] [timer.py:157:stop] 0/170, SamplesPerSec=3.4622767445229883\n","running epoch-0:   6%|‚ñå        | 179/3105 [00:53<14:35,  3.34it/s, tr_loss=1.61][2021-04-17 11:43:29,557] [INFO] [logging.py:60:log_dist] [Rank 0] step=180, skipped=0, lr=[5.5105450102066114e-05], mom=[[0.8, 0.999]]\n","running epoch-0:   6%|‚ñå        | 180/3105 [00:53<14:35,  3.34it/s, tr_loss=5.53][2021-04-17 11:43:29,725] [INFO] [timer.py:157:stop] 0/180, SamplesPerSec=3.4577395304347758\n","running epoch-0:   6%|‚ñå        | 189/3105 [00:56<14:28,  3.36it/s, tr_loss=2.81][2021-04-17 11:43:32,492] [INFO] [logging.py:60:log_dist] [Rank 0] step=190, skipped=0, lr=[5.5575072019056574e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:32,654] [INFO] [timer.py:157:stop] 0/190, SamplesPerSec=3.4586625690596677\n","running epoch-0:   6%|‚ñå        | 199/3105 [00:58<14:28,  3.35it/s, tr_loss=2.28][2021-04-17 11:43:35,453] [INFO] [logging.py:60:log_dist] [Rank 0] step=200, skipped=0, lr=[5.602059991327962e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:35,617] [INFO] [timer.py:157:stop] 0/200, SamplesPerSec=3.457378335715792\n","running epoch-0:   7%|‚ñå        | 209/3105 [01:01<14:01,  3.44it/s, tr_loss=6.89][2021-04-17 11:43:38,417] [INFO] [logging.py:60:log_dist] [Rank 0] step=210, skipped=0, lr=[5.644438589467838e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:38,585] [INFO] [timer.py:157:stop] 0/210, SamplesPerSec=3.457079406566462\n","running epoch-0:   7%|‚ñã        | 219/3105 [01:04<13:47,  3.49it/s, tr_loss=1.18][2021-04-17 11:43:41,346] [INFO] [logging.py:60:log_dist] [Rank 0] step=220, skipped=0, lr=[5.6848453616444126e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:41,515] [INFO] [timer.py:157:stop] 0/220, SamplesPerSec=3.4578306962628833\n","running epoch-0:   7%|‚ñã        | 229/3105 [01:07<14:37,  3.28it/s, tr_loss=1.36][2021-04-17 11:43:44,349] [INFO] [logging.py:60:log_dist] [Rank 0] step=230, skipped=0, lr=[5.7234556720351854e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:44,514] [INFO] [timer.py:157:stop] 0/230, SamplesPerSec=3.455472284436657\n","running epoch-0:   8%|‚ñã        | 239/3105 [01:10<14:25,  3.31it/s, tr_loss=5.26][2021-04-17 11:43:47,355] [INFO] [logging.py:60:log_dist] [Rank 0] step=240, skipped=0, lr=[5.7604224834232117e-05], mom=[[0.8, 0.999]]\n","[2021-04-17 11:43:47,523] [INFO] [timer.py:157:stop] 0/240, SamplesPerSec=3.452791397976927\n","running epoch-0:   8%|‚ñã        | 240/3105 [01:11<14:25,  3.31it/s, tr_loss=5.14]"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pLqqCPzHk_IT"},"source":[""],"execution_count":null,"outputs":[]}]}